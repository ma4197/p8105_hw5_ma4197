---
title: "Homework 5"
author: "Mayuri Albal"
date: "2022-11-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(rvest)
library(ggplot2)
```
##Problem 1


##Problem 2
```{r}
twp_data = read_csv(file= "./data/homicide-data.csv") %>%
janitor::clean_names()
```

**Describe the Raw Data**
Within the original data set there are 52,719 observations and 12 variables. The variables names include: 'uid', 'reported_date', 'victim_last', 'victim_first', 'victim_race', 'victim_age', 'victim_sex', 'city', 'state', 'lat', 'lon', and 'disposition'. This data was collected across different police department reports from 50 of the largest cities in the United States from the past decade.

City and State Variable Made
```{r}
murder_df=
twp_data %>%
janitor::clean_names() %>% 
mutate(
  state= replace(state, state == "AL", "OK"),
  state = replace(state, state== "wI", "WI")
) %>% 
mutate(
  city_state= paste(city, state, sep= ","))


```

Solved and Unsolved Case Count
```{r}
case_count= 
murder_df %>% 
  group_by(city_state) %>% 
  summarize(unsolved_n = sum(disposition== 'Closed without arrest',
                             disposition == 'Open/No arrest'),
                          total_m= n()) 
```

Prop.Test for Baltimore
```{r}
baltimore.test=
  case_count %>% 
  filter(city_state == "Baltimore,MD") %>% 
  transform(
    prop = map2(unsolved_n, total_m, ~prop.test(.x, .y) %>% 
      broom::tidy())) %>% 
    unnest() %>% 
  select(prop.estimate, prop.conf.low, prop.conf.high)
```

Prop.test All Cities
```{r}
all_cities = 
case_count %>% 
  mutate(
    prop= map2(unsolved_n, total_m, ~prop.test(.x, .y) %>% 
      broom::tidy())) %>% 
    unnest() %>% 
  select(city_state, estimate, conf.low, conf.high)

all_cities_output=
  all_cities %>% 
  knitr::kable() 
  
```

Plotting

```{r}
ggplot(all_cities, aes(x= reorder(city_state,estimate), y= estimate, color=city_state))+
  geom_point()+
  geom_errorbar(aes(ymin=conf.low, ymax= conf.high))+
  theme_classic()+ 
  theme(legend.position= "none")+
  ggtitle("Proportion Estimated of Unsolved Homicide in Large US Cities")+
  theme(plot.title = element_text(hjust= 0.5),
        axis.text.x.bottom = element_text(size= 7, angle= 90))+
    labs(x = "City",
        y= "Estimate of Proportion")
  
```
*Description*
The estimated proportions of unsolved homicides for each of the 50 largest cities in the US were graphed in order of lowest to highest proportion of unsolved cases. Richmond, VA has the lowest proportion, whilst Chicago, IL has the highest. Each was fitted with a error bar based upon the lowest and highest values from the estimates 95% confidence interval generated.

## Problem 3

Simulation #1
```{r}
simulate_prob3 = function(n = 30, mu=0, sigma = 5) {
  simulate_data =
  tibble(
    x = list(rnorm(n, mean = mu, sd = sigma)))
  
  return(simulate_data)}
  
output = tibble()
  for (i in 1:5000) {
    output = bind_rows(output,simulate_prob3(mu=0)) 
  }

ttest_output= output %>% 
  mutate(
    t_test_new = map(.x = x, ~t.test(.x) %>% broom::tidy())) %>% 
  unnest(t_test_new) %>% 
  select(x,estimate,p.value)
```


Simulation #2 with (1-6)
```{r}
simulate2 = function(n = 30, mu=0, sigma = 5) {
  simulate2_data =
  tibble(
    x = rnorm(n, mean = mu, sd = sigma))

simulate2_data %>% 
  t.test() %>% 
  broom::tidy()}
```

```{r}
mu_list=
  list(
    "mu = 1" = 1,
    "mu = 2" = 2,
    "mu = 3" = 3,
    "mu = 4" = 4,
    "mu = 5" = 5,
    "mu = 6" = 6
  )
mu_output= list(vector, length = 6)


for (i in 1:6) {
  mu_output[[i]] = rerun(5000, simulate2(mu = mu_list[[i]])) %>% 
    bind_rows()
}
```


```{r}
mu_output= tibble(mu_output)

mu = c(1,2,3,4,5,6)
mu_output ['mu']= mu
```

```{r}
mu_output_final=
  unnest(mu_output)
```

##Plots

```{r}
mu_final_plot=
mu_output_final %>% 
  group_by(mu) %>% 
  mutate(
    rnull= ifelse(p.value < 0.05, 1, 0)) %>% 
  summarise(pw = mean(rnull))
```

```{r}
ggplot(mu_final_plot, aes(x=mu, y=pw))+
  geom_point()+
  geom_smooth()+
  theme_classic()+ 
  theme(legend.position= "none")+
  ggtitle("True Mu Value versus Power Mu Values")+
    labs(x = "True Mu",
        y= "Power Mu Value")
```

*Description*
It is evident when observing the graph that as effect size (mu value) increases, the power increases as well. As a result of power being reliant upon both effect size and sample size, it is evident that there is a clear progression of how power increases as the effect size increases, due to the sample size remaining the same (n=30) throughout. There will be less power with smaller effect sizes due to them requiring a bigger sample size in order to achieve the same amount of power as those with large effect sizes. Yet with this, at a certain effect size, it appears to be around 4 for this data, power begins to plateau as the sample size is not changing.

Plot #2

Data Conditioning Plot #1
```{r}
mu_avg_plot =
mu_output_final %>% 
  group_by(mu) %>% 
  summarise(avg_mu = mean(estimate))
```

```{r}
ggplot(mu_avg_plot, aes(x= mu, y= avg_mu))+
  geom_point()+
  geom_line()+
  theme_classic()+ 
  ggtitle("Average Mu Value versus True Mu Value")+
    labs(x = "True Mu",
        y= "Average Mu Value")
```

Data Conditioning Plot #2

```{r}
mu_avg_null_plot =
mu_output_final %>% 
  filter(p.value <0.05) %>% 
  group_by(mu) %>% 
  summarise(avg_null_mu = mean(estimate))
```

```{r}
ggplot(mu_avg_null_plot, aes(x= mu, y= avg_null_mu))+
  geom_point()+
  geom_smooth()+
  theme_classic()+ 
  ggtitle("Average Mu Value versus True Mu Value Among Null Rejected Samples")+
    labs(x = "True Mu",
        y= "Average Mu Value")
```

**Description**
While the sample average mu across test samples where the null is rejected has approximately a linear relationship with the true mu, it is evident that the two are not equal. This can be evident through how many samples are able to reject the null hypothesis by getting a p-value less than 0.05, however they were not well crafted in other areas such as effect size or needed sample and as a result their true mu is quite different from the one found in the samples generated.